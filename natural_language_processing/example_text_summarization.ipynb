{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Summarization example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Links: [CNN DailyMail (HuggingFace)](https://huggingface.co/datasets/cnn_dailymail)  |  [Article (TowardsDataScience)](https://towardsdatascience.com/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import pckgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for W2V and textRank\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nlp_utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only extracted necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## for data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## for analysis\n",
    "import re\n",
    "import langdetect \n",
    "import nltk\n",
    "import wordcloud\n",
    "import contractions\n",
    "\n",
    "        # ## for sentiment\n",
    "        # from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "        # from textblob import TextBlob\n",
    "\n",
    "        # ## for ner\n",
    "        # import spacy\n",
    "        # import collections\n",
    "\n",
    " ## for machine learning\n",
    "from sklearn import preprocessing, model_selection, feature_extraction, feature_selection, metrics, manifold, naive_bayes, pipeline\n",
    "\n",
    "## for deep learning\n",
    "from tensorflow.keras import callbacks, models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "        # ## for explainer\n",
    "        # from lime import lime_text\n",
    "        # import shap\n",
    "\n",
    "        # ## for W2V and textRank\n",
    "        # import gensim\n",
    "        # import gensim.downloader as gensim_api\n",
    "\n",
    "        # ## for bert/bart\n",
    "        # import transformers\n",
    "\n",
    "## for summarization\n",
    "import rouge\n",
    "import difflib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cnn_dailymail/3.0.0 to C:/Users/Berenika/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 5/5 [00:00<00:00, 454.82it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] Proces nie może uzyskać dostępu do pliku, ponieważ jest on używany przez inny proces: 'C:/Users/Berenika/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de.incomplete\\\\cnn_dailymail-train-00000-00001-of-NNNNN.arrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\builder.py:1626\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[0;32m   1625\u001b[0m example \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39mencode_example(record) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m record\n\u001b[1;32m-> 1626\u001b[0m writer\u001b[39m.\u001b[39;49mwrite(example, key)\n\u001b[0;32m   1627\u001b[0m num_examples_progress_update \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\arrow_writer.py:488\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[1;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 488\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\arrow_writer.py:446\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m         batch_examples[col] \u001b[39m=\u001b[39m [\n\u001b[0;32m    443\u001b[0m             row[\u001b[39m0\u001b[39m][col]\u001b[39m.\u001b[39mto_pylist()[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row[\u001b[39m0\u001b[39m][col], (pa\u001b[39m.\u001b[39mArray, pa\u001b[39m.\u001b[39mChunkedArray)) \u001b[39melse\u001b[39;00m row[\u001b[39m0\u001b[39m][col]\n\u001b[0;32m    444\u001b[0m             \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples\n\u001b[0;32m    445\u001b[0m         ]\n\u001b[1;32m--> 446\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_batch(batch_examples\u001b[39m=\u001b[39;49mbatch_examples)\n\u001b[0;32m    447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\arrow_writer.py:555\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    554\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n\u001b[1;32m--> 555\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\arrow_writer.py:573\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[1;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_examples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mnum_rows\n\u001b[1;32m--> 573\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpa_writer\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\pyarrow\\ipc.pxi:525\u001b[0m, in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\fsspec\\implementations\\local.py:381\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf\u001b[39m.\u001b[39mwrite(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\builder.py:1635\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[0;32m   1634\u001b[0m num_shards \u001b[39m=\u001b[39m shard_id \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1635\u001b[0m num_examples, num_bytes \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39;49mfinalize()\n\u001b[0;32m   1636\u001b[0m writer\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\arrow_writer.py:582\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[1;34m(self, close_stream)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 582\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n\u001b[0;32m    583\u001b[0m \u001b[39m# If schema is known, infer features even if no examples were written\u001b[39;00m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\arrow_writer.py:446\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m         batch_examples[col] \u001b[39m=\u001b[39m [\n\u001b[0;32m    443\u001b[0m             row[\u001b[39m0\u001b[39m][col]\u001b[39m.\u001b[39mto_pylist()[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row[\u001b[39m0\u001b[39m][col], (pa\u001b[39m.\u001b[39mArray, pa\u001b[39m.\u001b[39mChunkedArray)) \u001b[39melse\u001b[39;00m row[\u001b[39m0\u001b[39m][col]\n\u001b[0;32m    444\u001b[0m             \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples\n\u001b[0;32m    445\u001b[0m         ]\n\u001b[1;32m--> 446\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_batch(batch_examples\u001b[39m=\u001b[39;49mbatch_examples)\n\u001b[0;32m    447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\arrow_writer.py:555\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    554\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39mTable\u001b[39m.\u001b[39mfrom_arrays(arrays, schema\u001b[39m=\u001b[39mschema)\n\u001b[1;32m--> 555\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\arrow_writer.py:573\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[1;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_examples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mnum_rows\n\u001b[1;32m--> 573\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpa_writer\u001b[39m.\u001b[39;49mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\pyarrow\\ipc.pxi:525\u001b[0m, in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\fsspec\\implementations\\local.py:381\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf\u001b[39m.\u001b[39mwrite(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\builder.py:842\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare.<locals>.incomplete_dir\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 842\u001b[0m     \u001b[39myield\u001b[39;00m tmp_dir\n\u001b[0;32m    843\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(dirname):\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\builder.py:890\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[1;32m--> 890\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    891\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager,\n\u001b[0;32m    892\u001b[0m         verification_mode\u001b[39m=\u001b[39mverification_mode,\n\u001b[0;32m    893\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    894\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    895\u001b[0m     )\n\u001b[0;32m    896\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\builder.py:1649\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[1;32m-> 1649\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1650\u001b[0m         dl_manager,\n\u001b[0;32m   1651\u001b[0m         verification_mode,\n\u001b[0;32m   1652\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39mverification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS\n\u001b[0;32m   1653\u001b[0m         \u001b[39mor\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS,\n\u001b[0;32m   1654\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs,\n\u001b[0;32m   1655\u001b[0m     )\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\builder.py:985\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split(split_generator, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs)\n\u001b[0;32m    986\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\builder.py:1487\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[0;32m   1486\u001b[0m \u001b[39mwith\u001b[39;00m pbar:\n\u001b[1;32m-> 1487\u001b[0m     \u001b[39mfor\u001b[39;00m job_id, done, content \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split_single(\n\u001b[0;32m   1488\u001b[0m         gen_kwargs\u001b[39m=\u001b[39mgen_kwargs, job_id\u001b[39m=\u001b[39mjob_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_prepare_split_args\n\u001b[0;32m   1489\u001b[0m     ):\n\u001b[0;32m   1490\u001b[0m         \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\builder.py:1644\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         e \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39m__context__\n\u001b[1;32m-> 1644\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetGenerationError(\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while generating the dataset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1646\u001b[0m \u001b[39myield\u001b[39;00m job_id, \u001b[39mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[39m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32me:\\Repozytoria\\DataScience_ArtificialIntelligence_Utils\\natural_language_processing\\example_text_summarization.ipynb Cell 12\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/example_text_summarization.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/example_text_summarization.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mload_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mcnn_dailymail\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m3.0.0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/example_text_summarization.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m lst_dics \u001b[39m=\u001b[39m [dic \u001b[39mfor\u001b[39;00m dic \u001b[39min\u001b[39;00m dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/example_text_summarization.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dtf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(lst_dics)\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhighlights\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m})[[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mhead(\u001b[39m20000\u001b[39m)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1794\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1796\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 1797\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   1798\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1799\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1800\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[0;32m   1801\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   1802\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m   1803\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   1804\u001b[0m )\n\u001b[0;32m   1806\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   1808\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   1809\u001b[0m )\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\builder.py:870\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_manual_download(dl_manager)\n\u001b[0;32m    869\u001b[0m \u001b[39m# Create a tmp dir and rename to self._output_dir on successful exit.\u001b[39;00m\n\u001b[1;32m--> 870\u001b[0m \u001b[39mwith\u001b[39;00m incomplete_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir) \u001b[39mas\u001b[39;00m tmp_output_dir:\n\u001b[0;32m    871\u001b[0m     \u001b[39m# Temporarily assign _output_dir to tmp_data_dir to avoid having to forward\u001b[39;00m\n\u001b[0;32m    872\u001b[0m     \u001b[39m# it to every sub function.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m     \u001b[39mwith\u001b[39;00m temporary_assignment(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_output_dir\u001b[39m\u001b[39m\"\u001b[39m, tmp_output_dir):\n\u001b[0;32m    874\u001b[0m         \u001b[39m# Try to download the already prepared dataset files\u001b[39;00m\n\u001b[0;32m    875\u001b[0m         downloaded_from_gcs \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32me:\\python\\lib\\contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    151\u001b[0m     value \u001b[39m=\u001b[39m typ()\n\u001b[0;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[0;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m value\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\datasets\\builder.py:849\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare.<locals>.incomplete_dir\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    848\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(tmp_dir):\n\u001b[1;32m--> 849\u001b[0m         shutil\u001b[39m.\u001b[39;49mrmtree(tmp_dir)\n",
      "File \u001b[1;32me:\\python\\lib\\shutil.py:739\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    737\u001b[0m     \u001b[39m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 739\u001b[0m \u001b[39mreturn\u001b[39;00m _rmtree_unsafe(path, onerror)\n",
      "File \u001b[1;32me:\\python\\lib\\shutil.py:617\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    615\u001b[0m             os\u001b[39m.\u001b[39munlink(fullname)\n\u001b[0;32m    616\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m--> 617\u001b[0m             onerror(os\u001b[39m.\u001b[39;49munlink, fullname, sys\u001b[39m.\u001b[39;49mexc_info())\n\u001b[0;32m    618\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     os\u001b[39m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32me:\\python\\lib\\shutil.py:615\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    614\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 615\u001b[0m         os\u001b[39m.\u001b[39;49munlink(fullname)\n\u001b[0;32m    616\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    617\u001b[0m         onerror(os\u001b[39m.\u001b[39munlink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] Proces nie może uzyskać dostępu do pliku, ponieważ jest on używany przez inny proces: 'C:/Users/Berenika/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de.incomplete\\\\cnn_dailymail-train-00000-00001-of-NNNNN.arrow'"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "lst_dics = [dic for dic in dataset[\"train\"]]\n",
    "\n",
    "dtf = pd.DataFrame(lst_dics).rename(columns={\"article\":\"text\", \"highlights\":\"y\"})[[\"text\",\"y\"]].head(20000)\n",
    "#dtf.to_csv(\"data_summary.csv\", index=False)\n",
    "#dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>It's official: U.S. President Barack Obama wan...</td>\n",
       "      <td>Syrian official: Obama climbed to the top of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(CNN) -- Usain Bolt rounded off the world cham...</td>\n",
       "      <td>Usain Bolt wins third gold of world championsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kansas City, Missouri (CNN) -- The General Ser...</td>\n",
       "      <td>The employee in agency's Kansas City office is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Los Angeles (CNN) -- A medical doctor in Vanco...</td>\n",
       "      <td>NEW: A Canadian doctor says she was part of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>(CNN) -- Police arrested another teen Thursday...</td>\n",
       "      <td>Another arrest made in gang rape outside Calif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  \\\n",
       "0           0  It's official: U.S. President Barack Obama wan...   \n",
       "1           1  (CNN) -- Usain Bolt rounded off the world cham...   \n",
       "2           2  Kansas City, Missouri (CNN) -- The General Ser...   \n",
       "3           3  Los Angeles (CNN) -- A medical doctor in Vanco...   \n",
       "4           4  (CNN) -- Police arrested another teen Thursday...   \n",
       "\n",
       "                                                   y  \n",
       "0  Syrian official: Obama climbed to the top of t...  \n",
       "1  Usain Bolt wins third gold of world championsh...  \n",
       "2  The employee in agency's Kansas City office is...  \n",
       "3  NEW: A Canadian doctor says she was part of a ...  \n",
       "4  Another arrest made in gang rape outside Calif...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf = pd.read_csv(\"data_summary.csv\")\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  20000 non-null  int64 \n",
      " 1   text        20000 non-null  object\n",
      " 2   y           20000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 468.9+ KB\n"
     ]
    }
   ],
   "source": [
    "dtf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Full text ---\n",
      "(CNN) -- If you travel by plane and arriving on time makes a difference, try to book on Hawaiian Airlines. In 2012, passengers got where they needed to go without delay on the carrier more than nine times out of 10, according to a study released on Monday. In fact, Hawaiian got even better from 2011, when it had a 92.8% on-time performance. Last year, it improved to 93.4%. The Airline Quality Rankings Report looks at the 14 largest U.S. airlines and is based on an analysis of U.S. Department of Transportation figures. It's co-authored by Brent Bowen, the head of the Department of Aviation Technology at Purdue University, and Dean Headley of Wichita State. In addition to on-time performance, the joint project looks at three other categories: rate of consumer complaints, mishandled bags and denied boarding performance. At a time when U.S. airlines are a whipping post for passenger complaints about crowded flights, tight seats, costly tickets and unsatisfactory service, there is a glimmer of hope. Eight airlines improved their on-time arrival performance in 2012. Nine of the 14 rated had an on-time arrival percentage of more than 80%. ExpressJet and American Airlines had the worst on-time performance (76.9%) last year, according to the data gathered in the 23rd annual report. Virgin America had the best baggage handling rate of all the airlines (0.87 misplaced bags per 1,000 passengers.) American Eagle showed improvement from 2011 but still came in last, fumbling baggage at a rate of 5.80 mishandled bags per 1,000 passengers. When it came to complaints last year, Southwest again had the lowest consumer rate (0.25 per 100,000 passengers) while the distinction of being the airline with the highest consumer complaint rate went to United Airlines (4.24 per 100,000.) Seven of the world's most entertaining airports . Boeing does 'final' battery test on 787 Dreamliner . FAA delays closures of 149 control towers .\n",
      " \n",
      "--- Summary ---\n",
      "Hawaiian Airlines again lands at No. 1 in on-time performance .\n",
      "The Airline Quality Rankings Report looks at the 14 largest U.S. airlines .\n",
      "ExpressJet and American Airlines had the worst on-time performance .\n",
      "Virgin America had the best baggage handling; Southwest had lowest complaint rate .\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "i = 10\n",
    "print(\"--- Full text ---\")\n",
    "print(dtf[\"text\"][i])\n",
    "print(\" \")\n",
    "print(\"--- Summary ---\")\n",
    "print(dtf[\"y\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text cleaning\n",
    "- Word frequency\n",
    "- Length analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Repozytoria\\DataScience_ArtificialIntelligence_Utils\\natural_language_processing\\example_text_summarization.ipynb Cell 19\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/example_text_summarization.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lst_stopwords \u001b[39m=\u001b[39m create_stopwords()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/example_text_summarization.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lst_stopwords \u001b[39m=\u001b[39m lst_stopwords \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcnn\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39msay\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39msaid\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mnew\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mwa\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mha\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "lst_stopwords = create_stopwords()\n",
    "lst_stopwords = lst_stopwords + [\"cnn\",\"say\",\"said\",\"new\",\"wa\",\"ha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf = add_preprocessed_text(dtf, column=\"text\", \n",
    "                            punkt=True, lower=True, slang=True, lst_stopwords=lst_stopwords, lemm=True)\n",
    "dtf = add_preprocessed_text(dtf, column=\"y\", \n",
    "                            punkt=True, lower=True, slang=True, lst_stopwords=lst_stopwords, lemm=True)\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "print(\"--- Full text ---\")\n",
    "print(dtf[\"text_clean\"][i])\n",
    "print(\" \")\n",
    "print(\"--- Summary ---\")\n",
    "print(dtf[\"y_clean\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common words in text\n",
    "dtf_freq = word_freq(corpus=dtf[\"text_clean\"], ngrams=[1], top=30, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 5\n",
    "X_top_words = len(dtf_freq[dtf_freq[\"freq\"]>thres])\n",
    "X_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common words in summaries\n",
    "dtf_freq = word_freq(corpus=dtf[\"y_clean\"], ngrams=[1], top=30, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 5\n",
    "y_top_words = len(dtf_freq[dtf_freq[\"freq\"]>thres])\n",
    "y_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texts\n",
    "X = add_text_length(dtf, \"text_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(X, x=\"word_count\", figsize=(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_len = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summaries\n",
    "y = add_text_length(dtf, \"y_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(y, x=\"word_count\", max_cat=1, figsize=(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_len = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf = dtf[[\"text\",\"text_clean\",\"y\",\"y_clean\"]]\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning\n",
    "dtf_train = dtf.iloc[i+1:]\n",
    "dtf_test = dtf.iloc[:i+1]\n",
    "dtf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Baseline (Extractive: TextRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test (no Train)\n",
    "predicted = textrank(corpus=dtf_test[\"text\"], ratio=y_len/X_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluate_summary(dtf_test[\"y\"][i], predicted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare y_test and predicted\n",
    "match = display_string_matching(dtf_test[\"y\"][i], predicted[i], both=True, sentences=False, \n",
    "                                titles=[\"Real Summary\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability\n",
    "match = display_string_matching(dtf_test[\"text\"][i], predicted[i], both=True, sentences=True, \n",
    "                                titles=[\"Full Text\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Model (Abstractive: Seq2Seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X_train for seq2seq (sequences of tokens)\n",
    "dic_seq = text2seq(corpus=dtf_train[\"text_clean\"], top=X_top_words, maxlen=X_len)\n",
    "\n",
    "X_train, X_tokenizer, X_dic_vocabulary = dic_seq[\"X\"], dic_seq[\"tokenizer\"], dic_seq[\"dic_vocabulary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(X_dic_vocabulary.items())[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess X_test with the same tokenizer\n",
    "X_test = text2seq(corpus=dtf_test[\"text_clean\"], fitted_tokenizer=X_tokenizer, maxlen=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add START and END tokens to the summaries (y)\n",
    "special_tokens = (\"<START>\", \"<END>\")\n",
    "dtf_train[\"y_clean\"] = dtf_train['y_clean'].apply(lambda x: special_tokens[0]+' '+x+' '+special_tokens[1])\n",
    "dtf_test[\"y_clean\"] = dtf_test['y_clean'].apply(lambda x: special_tokens[0]+' '+x+' '+special_tokens[1])\n",
    "dtf_test[\"y_clean\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the summaries (y_train)\n",
    "dic_seq = text2seq(corpus=dtf_train[\"y_clean\"], top=y_top_words, maxlen=y_len)\n",
    "\n",
    "y_train, y_tokenizer, y_dic_vocabulary = dic_seq[\"X\"], dic_seq[\"tokenizer\"], dic_seq[\"dic_vocabulary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(y_dic_vocabulary.items())[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess y_test with the same tokenizer\n",
    "y_test = text2seq(corpus=dtf_test[\"y_clean\"], fitted_tokenizer=y_tokenizer, maxlen=y_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec\n",
    "nlp = gensim_api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or train Word2Vec from scratch\n",
    "#corpus =  dtf_train[\"text_clean\"].append(dtf_train[\"y_clean\"])\n",
    "#lst_corpus, nlp = fit_w2v(corpus=corpus, min_count=1, size=300, window=y_avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check nlp model\n",
    "word = \"home\"\n",
    "nlp[word].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embeddings = vocabulary_embeddings(X_dic_vocabulary, nlp)\n",
    "X_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_embeddings = vocabulary_embeddings(y_dic_vocabulary, nlp)\n",
    "y_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Seq2Seq\n",
    "lstm_units = 250\n",
    "embeddings_size = 300\n",
    "\n",
    "##------------ ENCODER (embeddings + lstm) -----------------------------##\n",
    "x_in = layers.Input(name=\"x_in\", shape=(X_train.shape[1],))\n",
    "### embedding\n",
    "layer_x_emb = layers.Embedding(name=\"x_emb\", input_dim=len(X_dic_vocabulary), output_dim=embeddings_size, \n",
    "                               trainable=True)\n",
    "x_emb = layer_x_emb(x_in)\n",
    "### lstm \n",
    "layer_x_lstm = layers.LSTM(name=\"x_lstm\", units=lstm_units, dropout=0.4,  \n",
    "                           return_sequences=True, return_state=True)\n",
    "x_out, state_h, state_c = layer_x_lstm(x_emb)\n",
    "\n",
    "##------------ DECODER (embeddings + lstm + dense) ---------------------##\n",
    "y_in = layers.Input(name=\"y_in\", shape=(None,))\n",
    "### embedding\n",
    "layer_y_emb = layers.Embedding(name=\"y_emb\", input_dim=len(y_dic_vocabulary), output_dim=embeddings_size, \n",
    "                               trainable=True)\n",
    "y_emb = layer_y_emb(y_in)\n",
    "### lstm \n",
    "layer_y_lstm = layers.LSTM(name=\"y_lstm\", units=lstm_units, dropout=0.4,\n",
    "                           return_sequences=True, return_state=True)\n",
    "y_out, _, _ = layer_y_lstm(y_emb, initial_state=[state_h, state_c])\n",
    "### final dense layers\n",
    "layer_dense = layers.TimeDistributed(name=\"dense\", \n",
    "                                     layer=layers.Dense(units=len(y_dic_vocabulary), activation='softmax'))\n",
    "y_out = layer_dense(y_out)\n",
    "\n",
    "##---------------------------- COMPILE --------------------------------##\n",
    "model = models.Model(inputs=[x_in, y_in], outputs=y_out, name=\"Seq2Seq\")\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Seq2Seq\n",
    "lstm_units = 250\n",
    "\n",
    "##------------ ENCODER (pre-trained embeddings + 3 bi-lstm) ---------------##\n",
    "x_in = layers.Input(name=\"x_in\", shape=(X_train.shape[1],))\n",
    "### embedding\n",
    "layer_x_emb = layers.Embedding(name=\"x_emb\", input_dim=X_embeddings.shape[0], output_dim=X_embeddings.shape[1], \n",
    "                               weights=[X_embeddings], trainable=False)\n",
    "x_emb = layer_x_emb(x_in)\n",
    "### bi-lstm 1\n",
    "layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, dropout=0.4, recurrent_dropout=0.4,\n",
    "                                                  return_sequences=True, return_state=True), \n",
    "                                      name=\"x_lstm_1\")\n",
    "x_out, _, _, _, _ = layer_x_bilstm(x_emb)\n",
    "### bi-lstm 2\n",
    "layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, dropout=0.4, recurrent_dropout=0.4,\n",
    "                                                  return_sequences=True, return_state=True),\n",
    "                                      name=\"x_lstm_2\")\n",
    "x_out, _, _, _, _ = layer_x_bilstm(x_out)\n",
    "### bi-lstm 3 (here final states are collected)\n",
    "layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, dropout=0.4, recurrent_dropout=0.4, \n",
    "                                                  return_sequences=True, return_state=True),\n",
    "                                      name=\"x_lstm_3\")\n",
    "x_out, forward_h, forward_c, backward_h, backward_c = layer_x_bilstm(x_out)\n",
    "state_h = layers.Concatenate()([forward_h, backward_h])\n",
    "state_c = layers.Concatenate()([forward_c, backward_c])\n",
    "\n",
    "##------------ DECODER (pre-trained embeddings + lstm + dense) ------------##\n",
    "y_in = layers.Input(name=\"y_in\", shape=(None,))\n",
    "### embedding\n",
    "layer_y_emb = layers.Embedding(name=\"y_emb\", input_dim=y_embeddings.shape[0], output_dim=y_embeddings.shape[1], \n",
    "                               weights=[y_embeddings], trainable=False)\n",
    "y_emb = layer_y_emb(y_in)\n",
    "### lstm\n",
    "layer_y_lstm = layers.LSTM(name=\"y_lstm\", units=lstm_units*2, dropout=0.2, recurrent_dropout=0.2,\n",
    "                           return_sequences=True, return_state=True)\n",
    "y_out, _, _ = layer_y_lstm(y_emb, initial_state=[state_h, state_c])\n",
    "### final dense layers\n",
    "layer_dense = layers.TimeDistributed(name=\"dense\", \n",
    "                                     layer=layers.Dense(units=len(y_dic_vocabulary), activation='softmax'))\n",
    "y_out = layer_dense(y_out)\n",
    "\n",
    "##---------------------------- COMPILE ------------------------------------##\n",
    "model = models.Model(inputs=[x_in, y_in], outputs=y_out, name=\"Seq2Seq\")\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This takes a while\n",
    "model = fit_seq2seq(X_train, y_train, model, build_encoder_decoder=False, \n",
    "                    epochs=100, batch_size=64, verbose=1)\n",
    "#model, encoder_model, decoder_model = fit_seq2seq(X_train, y_train, X_embeddings, y_embeddings, model, \n",
    "#                                                  build_encoder_decoder=True, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Encoder\n",
    "encoder_model = models.Model(inputs=x_in, outputs=[x_out, state_h, state_c], name=\"Prediction_Encoder\")\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Decoder\n",
    "## double the lstm units if you used bidirectional lstm\n",
    "lstm_units = lstm_units*2 if any(\"Bidirectional\" in str(layer) for layer in model.layers) else lstm_units\n",
    "\n",
    "## states of the previous time step\n",
    "x_out2 = layers.Input(shape=(X_train.shape[1], lstm_units))\n",
    "state_h, state_c = layers.Input(shape=(lstm_units,)), layers.Input(shape=(lstm_units,))\n",
    "\n",
    "## decoder embeddings\n",
    "y_emb2 = layer_y_emb(y_in)\n",
    "\n",
    "## lstm to predict the next word\n",
    "y_out2, new_state_h, new_state_c = layer_y_lstm(y_emb2, initial_state=[state_h, state_c])\n",
    "\n",
    "## softmax to generate probability distribution over the target vocabulary\n",
    "probs = layer_dense(y_out2)\n",
    "\n",
    "## compile\n",
    "decoder_model = models.Model(inputs=[y_in, x_out2, state_h, state_c], \n",
    "                             outputs=[probs, new_state_h, new_state_c],\n",
    "                             name=\"Prediction_Decoder\")\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predict_seq2seq(X_test, encoder_model, decoder_model, y_tokenizer, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluate_summary(dtf_test[\"y_clean\"][i], predicted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare y_test and predicted\n",
    "match = display_string_matching(dtf_test[\"y_clean\"][i], predicted[i], both=True, sentences=False, \n",
    "                                titles=[\"Real Summary\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability\n",
    "match = display_string_matching(dtf_test[\"text_clean\"][i], predicted[i], both=True, sentences=False, \n",
    "                                titles=[\"Full Text\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Model (Abstractive: Language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test (no Train)\n",
    "predicted = bart(corpus=dtf_test[\"text\"], max_len=y_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluate_summary(dtf_test[\"y\"][i], predicted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare y_test and predicted\n",
    "match = display_string_matching(dtf_test[\"y\"][i], predicted[i], both=True, sentences=False, \n",
    "                                titles=[\"Real Summary\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability\n",
    "match = display_string_matching(dtf_test[\"text\"][i], predicted[i], both=True, sentences=True, \n",
    "                                titles=[\"Full Text\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
