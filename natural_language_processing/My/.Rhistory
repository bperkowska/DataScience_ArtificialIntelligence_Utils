library(rvest)
# Start by reading a HTML page with read_html():
starwars <- read_html("https://rvest.tidyverse.org/articles/starwars.html")
# Then find elements that match a css selector or XPath expression
# using html_elements(). In this example, each <section> corresponds
# to a different film
films <- starwars %>% html_elements("section")
films[1]
# so HERE WE SCRAPPED ENTIRE HTML CODE WITHIN <section>, </section>
# Then use html_element() to extract one element per film. Here
# we the title is given by the text inside <h2>
title <- films %>%
html_element("h2") %>%
html_text2()
title
# here the same but for description
description <- films %>% html_element("p") %>% html_text2()
description
films[1]
View(starwars)
starwars
starwars[1]
films[1]
# so HERE WE extracted these parts of code which are between <section>, </section>
class(html)
class(starwars)
# here the same but for description
release_dates <- films %>% html_elements("p") %>% html_text2()
release_dates
# here only first <p> using element, not elements
description <- films %>% html_element("p") %>% html_text2()
description
# WEB SCRAPPING - learning
library(rvest)
# Start by reading a HTML page with read_html():
starwars <- read_html("https://rvest.tidyverse.org/articles/starwars.html")
starwars # scrapped whole html code of a page
class(starwars)
# Then find elements that match a css selector or XPath expression using html_elements().
# In this example, each <section> corresponds to a different film
films <- starwars %>% html_elements("section")
films[1]
# so HERE WE extracted these parts of code which are between <section>, </section>
# Then use html_element() to extract one element per film. Here
# we the title is given by the text inside <h2>
title <- films %>%
html_element("h2") %>%
html_text2()
title
# here the same but for description
description <- films %>% html_elements("p") %>% html_text2()
description
# here only release_dates (first <p>), using element, not elements
release_dates <- films %>% html_element("p") %>% html_text2()
release_dates
# Or use html_attr() to get data out of attributes. html_attr() always
# returns a string so we convert it to an integer using a readr function
episode <- films %>%
html_element("h2") %>%
html_attr("data-id") %>%
readr::parse_integer()
episode
# read page html
page = xml2::read_html(url)
# for scrapping
library(xml2)
# read page html
page = xml2::read_html(url)
# SCRAPPING
url = "https://www.theguardian.com/world/2017/jun/26/angela-merkel-and-donald-trump-head-for-clash-at-g20-summit"
# read page html
page = xml2::read_html(url)
View(page)
page0 <- read_html(url)
View(page0)
View(page)
# extract text from page html using selector
page %>%
# extract paragraphs
rvest::html_nodes("p") %>%
# extract text
rvest::html_text() %>%
# remove empty elements
.[. != ""] -> text
#could also be written like this:
text0 <- page %>%
# extract paragraphs
rvest::html_nodes("p") %>%
# extract text
rvest::html_text() %>%
# remove empty elements
.[. != ""]
#could also be written like this:
text0 <- page0 %>%
# extract paragraphs
rvest::html_nodes("p") %>%
# extract text
rvest::html_text() %>%
# remove empty elements
.[. != ""]
text0
# Start by reading a HTML page with read_html():
starwars <- read_html("https://rvest.tidyverse.org/articles/starwars.html")
starwars # scrapped whole html code of a page
class(starwars)
# Then find elements that match a css selector using html_elements().
# In this example, each <section> corresponds to a different film
films <- starwars %>% html_elements("section")
films[1]
# Then use html_element() to extract one element per film. Here
# we the title is given by the text inside <h2>
title <- films %>%
html_element("h2") %>%
html_text2()
title
description
url = "https://towardsdatascience.com/data-entropy-more-data-more-problems-fa889a9dd0ec"
page0 <- read_html(url)
# extract text from page html using selector
page %>%
# extract paragraphs
rvest::html_nodes("p") %>%
# extract text
rvest::html_text() %>%
# remove empty elements
.[. != ""] -> text
# see data
head(text)
url = "https://towardsdatascience.com/data-entropy-more-data-more-problems-fa889a9dd0ec"
page0 <- read_html(url)
# read page html (two alternative ways; probably equivalent?)
page = xml2::read_html(url)
# extract text from page html using selector
page %>%
# extract paragraphs
rvest::html_nodes("p") %>%
# extract text
rvest::html_text() %>%
# remove empty elements
.[. != ""] -> text
# see data
head(text)
text
url = "https://medium.com/@ankitmarwaha18/understanding-the-basics-of-natural-language-processing-nlp-77849431cb05"
# read page html (two alternative ways; probably equivalent?)
page = xml2::read_html(url)
page0 <- read_html(url)
# extract text from page html using selector
page %>%
# extract paragraphs
rvest::html_nodes("p") %>%
# extract text
rvest::html_text() %>%
# remove empty elements
.[. != ""] -> text
# see data
head(text)
text
# see data
head(text)
# Then find elements that match a css selector using html_elements().
# In this example, each <section> corresponds to a different film
films <- starwars %>% html_elements("section p")
films[1]
films
# Then find elements that match a css selector using html_elements().
# In this example, each <section> corresponds to a different film
films <- starwars %>% html_elements("section")
films
url = "https://medium.com/@ankitmarwaha18/understanding-the-basics-of-natural-language-processing-nlp-77849431cb05"
page0 <- read_html(url)
View(page0)
page0
page0[2]
page0
library(html2txt)
library(htm2txt)
install.packages(htm2txt)
page0
print(page0[2])
print(page0$body)
print(page0)
print(page0[2])
head(page0[2])
display(page0[2])
# read page html (two alternative ways; probably equivalent?)
page = xml2::read_html(url)
page
# SCRAPPING
url = "https://www.theguardian.com/world/2017/jun/26/angela-merkel-and-donald-trump-head-for-clash-at-g20-summit"
# read page html (two alternative ways; probably equivalent?)
page = xml2::read_html(url)
page
# extract text from page html using selector
page %>%
# extract paragraphs
rvest::html_nodes("p") %>%
# extract text
rvest::html_text() %>%
# remove empty elements
.[. != ""] -> text
# see data
head(text)
url = "https://medium.com/@ankitmarwaha18/understanding-the-basics-of-natural-language-processing-nlp-77849431cb05"
text <- NLP %>% html_elements(p id="c4aa" class="pw-post-body-paragraph ld le fo lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma fh bj")
text <- NLP %>% html_elements(.pw-post-body-paragraph ld le fo lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma fh bj)
NLP <- read_html(url)
text <- NLP %>% html_elements(.pw-post-body-paragraph ld le fo lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma fh bj)
text <- NLP %>% html_elements(".pw-post-body-paragraph ld le fo lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma fh bj")
View(starwars)
View(text)
text <- NLP %>% html_elements(#8065 , #b3ac, #a690, #c4aa)
text <- NLP %>% html_elements("#8065")
text <- NLP %>% html_elements("#8065")
text <- NLP %>% html_elements(#8065)
text <- NLP %>% html_elements(.#8065)
text <- NLP %>% html_elements(//*[contains(concat( " ", @class, " " ), concat( " ", "dd", " " ))] | //ul)
text <- NLP %>% html_elements(//*[contains(concat( " ", @class, " " ), concat( " ", "dd", " " ))] | //ul)
url = "https://medium.com/@ankitmarwaha18/understanding-the-basics-of-natural-language-processing-nlp-77849431cb05"
NLP <- read_html(url)
text <- NLP %>% html_elements(//*[contains(concat( " ", @class, " " ), concat( " ", "dd", " " ))] | //ul)
text <- NLP %>% html_elements(/*[contains(concat( " ", @class, " " ), concat( " ", "dd", " " ))] | //ul)
text <- NLP %>% html_elements(*[contains(concat( " ", @class, " " ), concat( " ", "dd", " " ))] | //ul)
text <- NLP %>% html_elements([contains(concat( " ", @class, " " ), concat( " ", "dd", " " ))] | //ul)
text <- NLP %>% html_elements(ul)
text <- NLP %>% html_elements("ul")
View(text)
url = "view-source:https://www.theguardian.com/football/2023/may/20/nottingham-forest-arsenal-premier-league-match-report"
# read page html (two alternative ways; probably equivalent?)
page = xml2::read_html(url)
url = "https://www.theguardian.com/football/2023/may/20/nottingham-forest-arsenal-premier-league-match-report"
# read page html (two alternative ways; probably equivalent?)
page = xml2::read_html(url)
page
# extract text from page html using selector
page %>%
# extract paragraphs
rvest::html_nodes("p") %>%
# extract text
rvest::html_text() %>%
# remove empty elements
.[. != ""] -> text
# see data
head(text)
url = "https://www.theguardian.com/world/2023/may/18/italy-disasters-floods-europe-climate-crisis"
# read page html (two alternative ways; probably equivalent?)
page = xml2::read_html(url)
page
# extract text from page html using selector
page %>%
# extract paragraphs
rvest::html_nodes("p") %>%
# extract text
rvest::html_text() %>%
# remove empty elements
.[. != ""] -> text
# see data
head(text)
text
# see data
head(text)
# perform lexrank for top 3 sentences
top3sentences = lexRankr::lexRank(text,
# only 1 article; repeat same docid for all of input vector
docId = rep(1, length(text)),
# return 3 sentences
n = 3,
continuous = TRUE)
# inspect
top3sentences
# perform lexrank for top 3 sentences
top3sentences = lexRankr::lexRank(text,
# only 1 article; repeat same docid for all of input vector
docId = rep(1, length(text)),
# return 2 sentences
n = 2,
continuous = TRUE)
# inspect
top3sentences
top3sentences$sentence
# see data
head(text)
# for scrapping
library(rvest)
# for extractive summarization
library(lexRankr)
# SCRAPPING
url = "https://www.theguardian.com/world/2023/may/18/italy-disasters-floods-europe-climate-crisis"
page <- read_html(url)
page
text<- page %>%
rvest::html_nodes("p") %>%
rvest::html_text() %>%
.[. != ""]
# see data
# for scrapping
library(rvest)
# for extractive summarization
library(lexRankr)
# SCRAPPING
url = "https://www.theguardian.com/world/2023/may/18/italy-disasters-floods-europe-climate-crisis"
page <- read_html(url)
page
text<- page %>%
rvest::html_nodes("p") %>%
rvest::html_text() %>%
.[. != ""]
# see data
# see data
head(text)
text
# see data
head(text)
# perform lexrank
topSentences = lexRankr::lexRank(text,
# only 1 article; repeat same docid for all of input vector
docId = rep(1, length(text)),
# return 2 sentences
n = 2,
continuous = TRUE)
# see results
topSentences
topSentences$sentence
# for scrapping
library(rvest)
# for extractive summarization
library(lexRankr)
lexRankr
# for scrapping
library(rvest)
# for extractive summarization
library(lexRankr)
# SCRAPPING
url = "https://www.theguardian.com/world/2023/may/18/italy-disasters-floods-europe-climate-crisis"
page <- read_html(url)
page
text<- page %>%
rvest::html_nodes("p") %>%
rvest::html_text() %>%
.[. != ""]
# see data
head(text)
# see results
topSentences
topSentences$sentence
# perform lexrank
topSentences = lexRankr::lexRank(text,
# only 1 article; repeat same docid for all of input vector
docId = rep(1, length(text)),
# return 2 sentences
n = 2,
continuous = TRUE)
# see results
topSentences
topSentences$sentence
# perform lexrank
topSentences = lexRankr::lexRank(text,
# only 1 article; repeat same docid for all of input vector
docId = rep(1, length(text)),
# return 2 sentences
n = 3,
continuous = TRUE)
# see results
topSentences
topSentences$sentence
my_data <- read.delim("Italy.txt")
my_data <- readlines("Italy.txt")
my_data <- read.table("Italy.txt")
my_data <- paste0("Italy.txt")
my_data <- readtext("Italy.txt")
# for scrapping
library(rvest)
# for extractive summarization
library(lexRankr)
# SCRAPPING
url = "https://www.theguardian.com/world/2023/may/18/italy-disasters-floods-europe-climate-crisis"
page <- read_html(url)
page
text<- page %>%
rvest::html_nodes("p") %>%
rvest::html_text() %>%
.[. != ""]
# see data
head(text)
# The sentences are ranked based on their centrality in a graph.
# The graph is built upon the pairwise similarities of the sentences
# (where similarity is measured with a modified idf cosine similarity function).
# perform lexrank
topSentences = lexRankr::lexRank(text,
# only 1 article; repeat same docid for all of input vector
docId = rep(1, length(text)),
# return 2 sentences
n = 3,
continuous = TRUE)
# see results
topSentences
topSentences$sentence
