{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstractive summarization, SEQ2SEQ based on: <br> https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/#h-implementing-text-summarization-in-python-using-keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Summarization example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Links: [CNN DailyMail (HuggingFace)](https://huggingface.co/datasets/cnn_dailymail)  |  [Article (TowardsDataScience)](https://towardsdatascience.com/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import pckgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## for W2V and textRank\n",
    "# import gensim\n",
    "# import gensim.downloader as gensim_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only extracted necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## for data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## for analysis\n",
    "import re\n",
    "import langdetect \n",
    "import nltk\n",
    "import wordcloud\n",
    "import contractions\n",
    "\n",
    "        # ## for sentiment\n",
    "        # from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "        # from textblob import TextBlob\n",
    "\n",
    "        # ## for ner\n",
    "        # import spacy\n",
    "        # import collections\n",
    "\n",
    " ## for machine learning\n",
    "from sklearn import preprocessing, model_selection, feature_extraction, feature_selection, metrics, manifold, naive_bayes, pipeline\n",
    "\n",
    "## for deep learning\n",
    "from tensorflow.keras import callbacks, models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "        # ## for explainer\n",
    "        # from lime import lime_text\n",
    "        # import shap\n",
    "\n",
    "        # ## for W2V and textRank\n",
    "        # import gensim\n",
    "        # import gensim.downloader as gensim_api\n",
    "\n",
    "        # ## for bert/bart\n",
    "        # import transformers\n",
    "\n",
    "## for summarization\n",
    "import rouge\n",
    "import difflib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "lst_dics = [dic for dic in dataset[\"train\"]]\n",
    "\n",
    "dtf = pd.DataFrame(lst_dics).rename(columns={\"article\":\"text\", \"highlights\":\"y\"})[[\"text\",\"y\"]].head(20000)\n",
    "\n",
    "dtf.to_csv(\"data_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WASHINGTON (CNN) -- Doctors removed five small...</td>\n",
       "      <td>Five small polyps found during procedure; \"non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)  -- The National Football League has ind...</td>\n",
       "      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  LONDON, England (Reuters) -- Harry Potter star...   \n",
       "1  Editor's note: In our Behind the Scenes series...   \n",
       "2  MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n",
       "3  WASHINGTON (CNN) -- Doctors removed five small...   \n",
       "4  (CNN)  -- The National Football League has ind...   \n",
       "\n",
       "                                                   y  \n",
       "0  Harry Potter star Daniel Radcliffe gets £20M f...  \n",
       "1  Mentally ill inmates in Miami are housed on th...  \n",
       "2  NEW: \"I thought I was going to die,\" driver sa...  \n",
       "3  Five small polyps found during procedure; \"non...  \n",
       "4  NEW: NFL chief, Atlanta Falcons owner critical...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf = pd.read_csv(\"data_summary.csv\")\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    20000 non-null  object\n",
      " 1   y       20000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dtf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Full text ---\n",
      "WASHINGTON (CNN) -- As he awaits a crucial progress report on Iraq, President Bush will try to put a twist on comparisons of the war to Vietnam by invoking the historical lessons of that conflict to argue against pulling out. President Bush pauses Tuesday during a news conference at the  North American Leaders summit in Canada. On Wednesday in Kansas City, Missouri, Bush will tell members of the Veterans of Foreign Wars that \"then, as now, people argued that the real problem was America's presence and that if we would just withdraw, the killing would end,\" according to speech excerpts released Tuesday by the White House. \"Three decades later, there is a legitimate debate about how we got into the Vietnam War and how we left,\" Bush will say. \"Whatever your position in that debate, one unmistakable legacy of Vietnam is that the price of America's withdrawal was paid by millions of innocent citizens, whose agonies would add to our vocabulary new terms like 'boat people,' 're-education camps' and 'killing fields,' \" the president will say. The president will also make the argument that withdrawing from Vietnam emboldened today's terrorists by compromising U.S. credibility, citing a quote from al Qaeda leader Osama bin Laden that the American people would rise against the Iraq war the same way they rose against the war in Vietnam, according to the excerpts. \"Here at home, some can argue our withdrawal from Vietnam carried no price to American credibility, but the terrorists see things differently,\" Bush will say. On Tuesday, Democratic Senate Majority Leader Harry Reid said, \"President Bush's attempt to compare the war in Iraq to past military conflicts in East Asia ignores the fundamental difference between the two. Our nation was misled by the Bush Administration in an effort to gain support for the invasion of Iraq under false pretenses, leading to one of the worst foreign policy blunders in our history. \"While the President continues to stay-the-course with his failed strategy in Iraq, paid for by the taxpayers, American lives are being lost and there is still no political solution within the Iraqi government. It is time to change direction in Iraq, and Congress will again work to do so in the fall.\" The White House is billing the speech, along with another address next week to the American Legion, as an effort to \"provide broader context\" for the debate over the upcoming Iraq progress report by Gen. David Petraeus, the top U.S. military commander, and Ryan Crocker, the U.S. ambassador in Baghdad. President Bush has frequently asked lawmakers -- and the American people -- to withhold judgment on his troop \"surge\" in Iraq until the report comes out in September.  Watch Bush criticize the Iraqi government » . It is being closely watched on Capitol Hill, particularly by Republicans nervous about the political fallout from an increasingly unpopular war. Earlier this month, Defense Secretary Robert Gates said he would wait for the report before deciding when a drawdown of the 160,000 U.S. troops in Iraq might begin. Bush's speeches Wednesday and next week are the latest in a series of attempts by the White House to try to reframe the debate over Iraq, as public support for the war continues to sag. A recent CNN/Opinion Research Corporation poll found that almost two-thirds of Americans -- 64 percent -- now oppose the Iraq war, and 72 percent say that even if Petraeus reports progress, it won't change their opinion. The poll also found a great deal of skepticism about the report; 53 percent said they do not trust Petraeus to give an accurate assessment of the situation in Iraq. In addition to his analogy to Vietnam, Bush in Wednesday's speech will invoke other historical comparisons from Asia, including the U.S. defeat and occupation of Japan after World War II and the Korean War in the 1950s, according to the excerpts. \"In the aftermath of Japan's surrender, many thought it naive to help the Japanese transform themselves into a democracy. Then, as now, the critics argued that some people were simply not fit for freedom,\" Bush will say. \"Today, in defiance of the critics, Japan ... stands as one of the world's great free societies.\" Speaking about the Korean War, Bush will note that at the time \"critics argued that the war was futile, that we never should have sent our troops in, or that America's intervention was divisive here at home.\" \"While it is true that the Korean War had its share of challenges, America never broke its word,\" Bush will say. \"Without America's intervention during the war, and our willingness to stick with the South Koreans after the war, millions of South Koreans would now be living under a brutal and repressive regime.\" E-mail to a friend .\n",
      " \n",
      "--- Summary ---\n",
      "President Bush to address the Veterans of Foreign Wars on Wednesday .\n",
      "Bush to say that withdrawing from Vietnam emboldened today's terrorists .\n",
      "Speech will be latest White House attempt to try to reframe the debate over Iraq .\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "i = 10\n",
    "print(\"--- Full text ---\")\n",
    "print(dtf[\"text\"][i])\n",
    "print(\" \")\n",
    "print(\"--- Summary ---\")\n",
    "print(dtf[\"y\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train = dtf.iloc[i+1:]\n",
    "dtf_test = dtf.iloc[:i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text cleaning\n",
    "- Word frequency\n",
    "- Length analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_stopwords = create_stopwords()\n",
    "# lst_stopwords = lst_stopwords + [\"cnn\",\"say\",\"said\",\"new\",\"wa\",\"ha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_preprocessed_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Repozytoria\\DataScience_ArtificialIntelligence_Utils\\natural_language_processing\\My\\Seq2Seq.ipynb Cell 21\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dtf \u001b[39m=\u001b[39m add_preprocessed_text(dtf, column\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                             punkt\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, lower\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, slang\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, lst_stopwords\u001b[39m=\u001b[39mlst_stopwords, lemm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dtf \u001b[39m=\u001b[39m add_preprocessed_text(dtf, column\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                             punkt\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, lower\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, slang\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, lst_stopwords\u001b[39m=\u001b[39mlst_stopwords, lemm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dtf\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'add_preprocessed_text' is not defined"
     ]
    }
   ],
   "source": [
    "dtf = add_preprocessed_text(dtf, column=\"text\", \n",
    "                            punkt=True, lower=True, slang=True, lst_stopwords=lst_stopwords, lemm=True)\n",
    "dtf = add_preprocessed_text(dtf, column=\"y\", \n",
    "                            punkt=True, lower=True, slang=True, lst_stopwords=lst_stopwords, lemm=True)\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Full text ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text_clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_clean'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32me:\\Repozytoria\\DataScience_ArtificialIntelligence_Utils\\natural_language_processing\\My\\Seq2Seq.ipynb Cell 22\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# check\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m--- Full text ---\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(dtf[\u001b[39m\"\u001b[39;49m\u001b[39mtext_clean\u001b[39;49m\u001b[39m\"\u001b[39;49m][i])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m--- Summary ---\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_clean'"
     ]
    }
   ],
   "source": [
    "# check\n",
    "print(\"--- Full text ---\")\n",
    "print(dtf[\"text_clean\"][i])\n",
    "print(\" \")\n",
    "print(\"--- Summary ---\")\n",
    "print(dtf[\"y_clean\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Repozytoria\\DataScience_ArtificialIntelligence_Utils\\natural_language_processing\\My\\Seq2Seq.ipynb Cell 25\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Find most common words in text\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dtf_freq \u001b[39m=\u001b[39m word_freq(corpus\u001b[39m=\u001b[39mdtf[\u001b[39m\"\u001b[39m\u001b[39mtext_clean\u001b[39m\u001b[39m\"\u001b[39m], ngrams\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m], top\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,\u001b[39m7\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_freq' is not defined"
     ]
    }
   ],
   "source": [
    "# Find most common words in text\n",
    "dtf_freq = word_freq(corpus=dtf[\"text_clean\"], ngrams=[1], top=30, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 5\n",
    "X_top_words = len(dtf_freq[dtf_freq[\"freq\"]>thres])\n",
    "X_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common words in summaries\n",
    "dtf_freq = word_freq(corpus=dtf[\"y_clean\"], ngrams=[1], top=30, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 5\n",
    "y_top_words = len(dtf_freq[dtf_freq[\"freq\"]>thres])\n",
    "y_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texts\n",
    "X = add_text_length(dtf, \"text_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(X, x=\"word_count\", figsize=(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_len = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summaries\n",
    "y = add_text_length(dtf, \"y_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(y, x=\"word_count\", max_cat=1, figsize=(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_len = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf = dtf[[\"text\",\"text_clean\",\"y\",\"y_clean\"]]\n",
    "dtf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning\n",
    "dtf_train = dtf.iloc[i+1:]\n",
    "dtf_test = dtf.iloc[:i+1]\n",
    "dtf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Baseline (Extractive: TextRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test (no Train)\n",
    "predicted = textrank(corpus=dtf_test[\"text\"], ratio=y_len/X_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluate_summary(dtf_test[\"y\"][i], predicted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare y_test and predicted\n",
    "match = display_string_matching(dtf_test[\"y\"][i], predicted[i], both=True, sentences=False, \n",
    "                                titles=[\"Real Summary\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability\n",
    "match = display_string_matching(dtf_test[\"text\"][i], predicted[i], both=True, sentences=True, \n",
    "                                titles=[\"Full Text\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Model (Abstractive: Seq2Seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X_train for seq2seq (sequences of tokens)\n",
    "dic_seq = text2seq(corpus=dtf_train[\"text_clean\"], top=X_top_words, maxlen=X_len)\n",
    "\n",
    "X_train, X_tokenizer, X_dic_vocabulary = dic_seq[\"X\"], dic_seq[\"tokenizer\"], dic_seq[\"dic_vocabulary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(X_dic_vocabulary.items())[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess X_test with the same tokenizer\n",
    "X_test = text2seq(corpus=dtf_test[\"text_clean\"], fitted_tokenizer=X_tokenizer, maxlen=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add START and END tokens to the summaries (y)\n",
    "special_tokens = (\"<START>\", \"<END>\")\n",
    "dtf_train[\"y_clean\"] = dtf_train['y_clean'].apply(lambda x: special_tokens[0]+' '+x+' '+special_tokens[1])\n",
    "dtf_test[\"y_clean\"] = dtf_test['y_clean'].apply(lambda x: special_tokens[0]+' '+x+' '+special_tokens[1])\n",
    "dtf_test[\"y_clean\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the summaries (y_train)\n",
    "dic_seq = text2seq(corpus=dtf_train[\"y_clean\"], top=y_top_words, maxlen=y_len)\n",
    "\n",
    "y_train, y_tokenizer, y_dic_vocabulary = dic_seq[\"X\"], dic_seq[\"tokenizer\"], dic_seq[\"dic_vocabulary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(y_dic_vocabulary.items())[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess y_test with the same tokenizer\n",
    "y_test = text2seq(corpus=dtf_test[\"y_clean\"], fitted_tokenizer=y_tokenizer, maxlen=y_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec\n",
    "nlp = gensim_api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or train Word2Vec from scratch\n",
    "#corpus =  dtf_train[\"text_clean\"].append(dtf_train[\"y_clean\"])\n",
    "#lst_corpus, nlp = fit_w2v(corpus=corpus, min_count=1, size=300, window=y_avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check nlp model\n",
    "word = \"home\"\n",
    "nlp[word].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embeddings = vocabulary_embeddings(X_dic_vocabulary, nlp)\n",
    "X_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_embeddings = vocabulary_embeddings(y_dic_vocabulary, nlp)\n",
    "y_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Seq2Seq\n",
    "lstm_units = 250\n",
    "embeddings_size = 300\n",
    "\n",
    "##------------ ENCODER (embeddings + lstm) -----------------------------##\n",
    "x_in = layers.Input(name=\"x_in\", shape=(X_train.shape[1],))\n",
    "### embedding\n",
    "layer_x_emb = layers.Embedding(name=\"x_emb\", input_dim=len(X_dic_vocabulary), output_dim=embeddings_size, \n",
    "                               trainable=True)\n",
    "x_emb = layer_x_emb(x_in)\n",
    "### lstm \n",
    "layer_x_lstm = layers.LSTM(name=\"x_lstm\", units=lstm_units, dropout=0.4,  \n",
    "                           return_sequences=True, return_state=True)\n",
    "x_out, state_h, state_c = layer_x_lstm(x_emb)\n",
    "\n",
    "##------------ DECODER (embeddings + lstm + dense) ---------------------##\n",
    "y_in = layers.Input(name=\"y_in\", shape=(None,))\n",
    "### embedding\n",
    "layer_y_emb = layers.Embedding(name=\"y_emb\", input_dim=len(y_dic_vocabulary), output_dim=embeddings_size, \n",
    "                               trainable=True)\n",
    "y_emb = layer_y_emb(y_in)\n",
    "### lstm \n",
    "layer_y_lstm = layers.LSTM(name=\"y_lstm\", units=lstm_units, dropout=0.4,\n",
    "                           return_sequences=True, return_state=True)\n",
    "y_out, _, _ = layer_y_lstm(y_emb, initial_state=[state_h, state_c])\n",
    "### final dense layers\n",
    "layer_dense = layers.TimeDistributed(name=\"dense\", \n",
    "                                     layer=layers.Dense(units=len(y_dic_vocabulary), activation='softmax'))\n",
    "y_out = layer_dense(y_out)\n",
    "\n",
    "##---------------------------- COMPILE --------------------------------##\n",
    "model = models.Model(inputs=[x_in, y_in], outputs=y_out, name=\"Seq2Seq\")\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Seq2Seq\n",
    "lstm_units = 250\n",
    "\n",
    "##------------ ENCODER (pre-trained embeddings + 3 bi-lstm) ---------------##\n",
    "x_in = layers.Input(name=\"x_in\", shape=(X_train.shape[1],))\n",
    "### embedding\n",
    "layer_x_emb = layers.Embedding(name=\"x_emb\", input_dim=X_embeddings.shape[0], output_dim=X_embeddings.shape[1], \n",
    "                               weights=[X_embeddings], trainable=False)\n",
    "x_emb = layer_x_emb(x_in)\n",
    "### bi-lstm 1\n",
    "layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, dropout=0.4, recurrent_dropout=0.4,\n",
    "                                                  return_sequences=True, return_state=True), \n",
    "                                      name=\"x_lstm_1\")\n",
    "x_out, _, _, _, _ = layer_x_bilstm(x_emb)\n",
    "### bi-lstm 2\n",
    "layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, dropout=0.4, recurrent_dropout=0.4,\n",
    "                                                  return_sequences=True, return_state=True),\n",
    "                                      name=\"x_lstm_2\")\n",
    "x_out, _, _, _, _ = layer_x_bilstm(x_out)\n",
    "### bi-lstm 3 (here final states are collected)\n",
    "layer_x_bilstm = layers.Bidirectional(layers.LSTM(units=lstm_units, dropout=0.4, recurrent_dropout=0.4, \n",
    "                                                  return_sequences=True, return_state=True),\n",
    "                                      name=\"x_lstm_3\")\n",
    "x_out, forward_h, forward_c, backward_h, backward_c = layer_x_bilstm(x_out)\n",
    "state_h = layers.Concatenate()([forward_h, backward_h])\n",
    "state_c = layers.Concatenate()([forward_c, backward_c])\n",
    "\n",
    "##------------ DECODER (pre-trained embeddings + lstm + dense) ------------##\n",
    "y_in = layers.Input(name=\"y_in\", shape=(None,))\n",
    "### embedding\n",
    "layer_y_emb = layers.Embedding(name=\"y_emb\", input_dim=y_embeddings.shape[0], output_dim=y_embeddings.shape[1], \n",
    "                               weights=[y_embeddings], trainable=False)\n",
    "y_emb = layer_y_emb(y_in)\n",
    "### lstm\n",
    "layer_y_lstm = layers.LSTM(name=\"y_lstm\", units=lstm_units*2, dropout=0.2, recurrent_dropout=0.2,\n",
    "                           return_sequences=True, return_state=True)\n",
    "y_out, _, _ = layer_y_lstm(y_emb, initial_state=[state_h, state_c])\n",
    "### final dense layers\n",
    "layer_dense = layers.TimeDistributed(name=\"dense\", \n",
    "                                     layer=layers.Dense(units=len(y_dic_vocabulary), activation='softmax'))\n",
    "y_out = layer_dense(y_out)\n",
    "\n",
    "##---------------------------- COMPILE ------------------------------------##\n",
    "model = models.Model(inputs=[x_in, y_in], outputs=y_out, name=\"Seq2Seq\")\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a while\n",
    "model = fit_seq2seq(X_train, y_train, model, build_encoder_decoder=False, \n",
    "                    epochs=100, batch_size=64, verbose=1)\n",
    "#model, encoder_model, decoder_model = fit_seq2seq(X_train, y_train, X_embeddings, y_embeddings, model, \n",
    "#                                                  build_encoder_decoder=True, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Encoder\n",
    "encoder_model = models.Model(inputs=x_in, outputs=[x_out, state_h, state_c], name=\"Prediction_Encoder\")\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Decoder\n",
    "## double the lstm units if you used bidirectional lstm\n",
    "lstm_units = lstm_units*2 if any(\"Bidirectional\" in str(layer) for layer in model.layers) else lstm_units\n",
    "\n",
    "## states of the previous time step\n",
    "x_out2 = layers.Input(shape=(X_train.shape[1], lstm_units))\n",
    "state_h, state_c = layers.Input(shape=(lstm_units,)), layers.Input(shape=(lstm_units,))\n",
    "\n",
    "## decoder embeddings\n",
    "y_emb2 = layer_y_emb(y_in)\n",
    "\n",
    "## lstm to predict the next word\n",
    "y_out2, new_state_h, new_state_c = layer_y_lstm(y_emb2, initial_state=[state_h, state_c])\n",
    "\n",
    "## softmax to generate probability distribution over the target vocabulary\n",
    "probs = layer_dense(y_out2)\n",
    "\n",
    "## compile\n",
    "decoder_model = models.Model(inputs=[y_in, x_out2, state_h, state_c], \n",
    "                             outputs=[probs, new_state_h, new_state_c],\n",
    "                             name=\"Prediction_Decoder\")\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predict_seq2seq(X_test, encoder_model, decoder_model, y_tokenizer, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluate_summary(dtf_test[\"y_clean\"][i], predicted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare y_test and predicted\n",
    "match = display_string_matching(dtf_test[\"y_clean\"][i], predicted[i], both=True, sentences=False, \n",
    "                                titles=[\"Real Summary\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability\n",
    "match = display_string_matching(dtf_test[\"text_clean\"][i], predicted[i], both=True, sentences=False, \n",
    "                                titles=[\"Full Text\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Model (Abstractive: Language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test (no Train)\n",
    "predicted = bart(corpus=dtf_test[\"text\"], max_len=y_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluate_summary(dtf_test[\"y\"][i], predicted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare y_test and predicted\n",
    "match = display_string_matching(dtf_test[\"y\"][i], predicted[i], both=True, sentences=False, \n",
    "                                titles=[\"Real Summary\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability\n",
    "match = display_string_matching(dtf_test[\"text\"][i], predicted[i], both=True, sentences=True, \n",
    "                                titles=[\"Full Text\", \"Predicted Summary\"])\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(match))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Ze strony (stąd: https://towardsdatascience.com/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Berenika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'cnn',\n",
       " 'say',\n",
       " 'said',\n",
       " 'new']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "## create stopwords\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "## add words that are too frequent\n",
    "lst_stopwords = lst_stopwords + [\"cnn\",\"say\",\"said\",\"new\"]\n",
    "\n",
    "lst_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Berenika\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## cleaning function\n",
    "def utils_preprocess_text(txt, punkt=True, lower=True, slang=True, lst_stopwords=None, stemm=False, lemm=True):\n",
    "    ### separate sentences with '. '\n",
    "    txt = re.sub(r'\\.(?=[^ \\W\\d])', '. ', str(txt))\n",
    "    ### remove punctuations and characters\n",
    "    txt = re.sub(r'[^\\w\\s]', '', txt) if punkt is True else txt\n",
    "    ### strip\n",
    "    txt = \" \".join([word.strip() for word in txt.split()])\n",
    "    ### lowercase\n",
    "    txt = txt.lower() if lower is True else txt\n",
    "    ### slang\n",
    "    txt = contractions.fix(txt) if slang is True else txt   \n",
    "    ### tokenize (convert from string to list)\n",
    "    lst_txt = txt.split()\n",
    "    ### stemming (remove -ing, -ly, ...)\n",
    "    if stemm is True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_txt = [ps.stem(word) for word in lst_txt]\n",
    "    ### lemmatization (convert the word into root word)\n",
    "    if lemm is True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_txt = [lem.lemmatize(word) for word in lst_txt]\n",
    "    ### remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_txt = [word for word in lst_txt if word not in \n",
    "                   lst_stopwords]\n",
    "    ### back to string\n",
    "    txt = \" \".join(lst_txt)\n",
    "    return txt\n",
    "\n",
    "## apply function to both text and summaries\n",
    "dtf_train[\"text_clean\"] = dtf_train[\"text\"].apply(lambda x: utils_preprocess_text(x, punkt=True, lower=True, slang=True, lst_stopwords=lst_stopwords, stemm=False, lemm=True))\n",
    "dtf_train[\"y_clean\"] = dtf_train[\"y\"].apply(lambda x: utils_preprocess_text(x, punkt=True, lower=True, slang=True, lst_stopwords=lst_stopwords, stemm=False, lemm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>y_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LONDON, England (CNN) -- A chronology of bombi...</td>\n",
       "      <td>Two cars loaded with gasoline and nails found ...</td>\n",
       "      <td>london england chronology bombing attempted bo...</td>\n",
       "      <td>two car loaded gasoline nail found abandoned l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BREMEN, Germany -- Carlos Alberto, who scored ...</td>\n",
       "      <td>Werder Bremen pay a club record $10.7 million ...</td>\n",
       "      <td>bremen germany carlos alberto scored fc porto ...</td>\n",
       "      <td>werder bremen pay club record 107 million carl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WASHINGTON (CNN) -- Vice President Dick Cheney...</td>\n",
       "      <td>President Bush will have a routine colonoscopy...</td>\n",
       "      <td>washington vice president dick cheney serve ac...</td>\n",
       "      <td>president bush routine colonoscopy saturday an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SAN FRANCISCO, California (CNN)  -- A magnitud...</td>\n",
       "      <td>2,000 customers without electricity, power com...</td>\n",
       "      <td>san francisco california magnitude 42 earthqua...</td>\n",
       "      <td>2000 customer without electricity power compan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WASHINGTON (CNN) -- There is \"no remaining hop...</td>\n",
       "      <td>NEW: President Bush says he and first lady are...</td>\n",
       "      <td>washington remaining hope finding six men trap...</td>\n",
       "      <td>president bush first lady deeply saddened trag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "11  LONDON, England (CNN) -- A chronology of bombi...   \n",
       "12  BREMEN, Germany -- Carlos Alberto, who scored ...   \n",
       "13  WASHINGTON (CNN) -- Vice President Dick Cheney...   \n",
       "14  SAN FRANCISCO, California (CNN)  -- A magnitud...   \n",
       "15  WASHINGTON (CNN) -- There is \"no remaining hop...   \n",
       "\n",
       "                                                    y  \\\n",
       "11  Two cars loaded with gasoline and nails found ...   \n",
       "12  Werder Bremen pay a club record $10.7 million ...   \n",
       "13  President Bush will have a routine colonoscopy...   \n",
       "14  2,000 customers without electricity, power com...   \n",
       "15  NEW: President Bush says he and first lady are...   \n",
       "\n",
       "                                           text_clean  \\\n",
       "11  london england chronology bombing attempted bo...   \n",
       "12  bremen germany carlos alberto scored fc porto ...   \n",
       "13  washington vice president dick cheney serve ac...   \n",
       "14  san francisco california magnitude 42 earthqua...   \n",
       "15  washington remaining hope finding six men trap...   \n",
       "\n",
       "                                              y_clean  \n",
       "11  two car loaded gasoline nail found abandoned l...  \n",
       "12  werder bremen pay club record 107 million carl...  \n",
       "13  president bush routine colonoscopy saturday an...  \n",
       "14  2000 customer without electricity power compan...  \n",
       "15  president bush first lady deeply saddened trag...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Berenika/nltk_data'\n    - 'e:\\\\python\\\\nltk_data'\n    - 'e:\\\\python\\\\share\\\\nltk_data'\n    - 'e:\\\\python\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Berenika\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32me:\\Repozytoria\\DataScience_ArtificialIntelligence_Utils\\natural_language_processing\\My\\Seq2Seq.ipynb Cell 82\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#Y162sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## count\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#Y162sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dtf_train[\u001b[39m'\u001b[39m\u001b[39mword_count\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dtf_train\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39mlen\u001b[39;49m(nltk\u001b[39m.\u001b[39;49mword_tokenize(\u001b[39mstr\u001b[39;49m(x))) )\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\pandas\\core\\frame.py:9558\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9547\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9549\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   9550\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   9551\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9556\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   9557\u001b[0m )\n\u001b[1;32m-> 9558\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\pandas\\core\\apply.py:741\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 741\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\pandas\\core\\apply.py:868\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 868\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    870\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\pandas\\core\\apply.py:884\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    882\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    883\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 884\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    885\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    886\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    887\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    888\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32me:\\Repozytoria\\DataScience_ArtificialIntelligence_Utils\\natural_language_processing\\My\\Seq2Seq.ipynb Cell 82\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#Y162sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## count\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Repozytoria/DataScience_ArtificialIntelligence_Utils/natural_language_processing/My/Seq2Seq.ipynb#Y162sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dtf_train[\u001b[39m'\u001b[39m\u001b[39mword_count\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dtf_train\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(nltk\u001b[39m.\u001b[39;49mword_tokenize(\u001b[39mstr\u001b[39;49m(x))) )\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Berenika/nltk_data'\n    - 'e:\\\\python\\\\nltk_data'\n    - 'e:\\\\python\\\\share\\\\nltk_data'\n    - 'e:\\\\python\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Berenika\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "## count\n",
    "dtf_train['word_count'] = dtf_train.apply(lambda x: len(nltk.word_tokenize(str(x))) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot\n",
    "sns.distplot(dtf_train[\"word_count\"], hist=True, kde=True, kde_kws={\"shade\":True})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
